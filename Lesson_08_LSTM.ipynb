{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lesson_08-LSTM.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "neural1",
      "language": "python",
      "name": "neural1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fisherj1/Neural/blob/Lesson8/Lesson_08_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "# Генерация текста с использованием LSTM сетей\n",
        "Импортируем numpy, чтобы преобразовывать буквы в числа, да и вообще это must-have. tensorflow это понятно, а os будет нужен, чтобы работать с путями к файлам."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import urllib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBs9V_UMPZUY"
      },
      "source": [
        "os.mkdir(\"input_data/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWxS95k9OPNo"
      },
      "source": [
        "resource = urllib.request.urlopen(\"http://t.stelm.ru/nn/lesson7/frankenstein.txt\")\n",
        "out = open(\"input_data/frankenstein.txt\", 'wb')\n",
        "out.write(resource.read())\n",
        "out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "Далее откроем локально приложенный текст, на котором мы будем обучаться. Приводится текст на английском языке, что влияет на коды букв и фильтрацию лишних символов. При открытии текстового файла надо учитывать кодировку. Нельзя просто считать один байт одной буквой. Некоторые кодировки могут обозначать буквы двумя байтами. Проще всего выбрать кодировку UTF-8, убедиться в текстовом редакторе, что файл сохранён в ней и использовать эту кодировку при открытии файла."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "source": [
        "path_to_file = \"input_data/frankenstein.txt\"\n",
        "\n",
        "# Открыть файл на чтение в бинарном виде, считать и декодировать\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8UaL79oMdyr"
      },
      "source": [
        "### Фикс для GPU\n",
        "Для видеокарт с 4 ГБ памяти и меньше tensorflow оставляет недостаточный запас свободной видеопамяти, поэтому включим выделение памяти по требованию, вместо дефолтной процеду выделения сразу почти всей памяти."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE6u6KZzMdys"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Проверим текст\n",
        "Мы ожидаем, что текст корректно считался, что в нём менее 80 символов (для текстов на английском), включающих в себя строчные и прописные буквы (26*2 для английского алфавита),  пунктуацию(18), символы переноса строки (2). Для проверки текста сделаем функцию, которая выведет начало текста на экран, посчитает количество символов в нём, посчитаем уникальные символы и выведем их. Также сразу выделим символы за пределами стандартного алфавита, цифр и пунктуации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavnuByVymwK"
      },
      "source": [
        "def check_text(text):\n",
        "    # Посмотрим первые 300 символов\n",
        "    print(text[:300])\n",
        "    # Посмотрим общее количество символов\n",
        "    print ('\\nLength of text: {} characters'.format(len(text)))\n",
        "    # Построим перечень уникальных символов, а numpy поможет нам работать с массивом\n",
        "    vocab = np.array(sorted(set(text)))\n",
        "    # Выведем на экран все уникальные символы\n",
        "    print ('{} unique characters:'.format(len(vocab)))\n",
        "    print(vocab)\n",
        "    # Нестандартные специальные символы в английском начинаются после символа 'z'\n",
        "    print ('Bad characters in english text:')\n",
        "    print(vocab[vocab > 'z'])\n",
        "    return vocab\n",
        "\n",
        "vocab = check_text(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxOmWUL6Mdyx"
      },
      "source": [
        "Если в тексте есть плохие символы (нестандартные специальные символы), то стоит их отфильтровать из текста. Для этого берем последний символ, после которого начинаются специальные символы и уберем их из текста. Это не совсем корректно, ведь это могли быть, например, буквы с поставленными ударениями, которые нужно заменить на буквы без ударений. Но для простоты и универсальности - просто удалим лишнее."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F08rtamMdyx"
      },
      "source": [
        "# Удаляем символы после последнего значимого\n",
        "text = text.translate({ord(c): None for c in vocab[vocab > 'z']}) # As recommended in https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python\n",
        "# Снова смотрим на текст, на словарь и пересохраняем его себе\n",
        "vocab = check_text(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Перевод символов в числа и наоборот\n",
        "\n",
        "Теперь текст должен состоять только из хороших символов и можно продолжать. Но для нейронной сети нам нужны числа, а не символы. Сделаем это с помощью двух таблиц-словарей. Вообще одна таблица у нас уже есть - наш словарь символов. По индексу он выдаёт символ. Можно сделать обратный к нему словарь. Далее переведем текст в целые числа (индексы словаря). В конце еще сделаем тип массива этих чисел int8, так как у нас небольшой словарь, а по умолчанию будет использоваться int64, что в 8 раз больше тратит памяти."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IalZLbvOzf-F"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = vocab\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text]).astype('uint8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ELULjvOMdy2"
      },
      "source": [
        "Проверим, что текст в виде индексов соответствует тексту в виде символов. Для этого выведем их вместе."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1VKcQHcymwb"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Создание обучающей выборки\n",
        "\n",
        "Наша цель в обучающей выборке это выделять из текста пары строк определённой длины (`seq_length`), где вторая строка это первая, сдвинутая на один символ вперед. Первая строка это вход, а вторая это ожидаемый выход, который должен уметь предсказывать текст на один символ вперед. Так мы разобьём весь текст. Для этого мы будем использовать `seq_length+1`, чтобы эта длина покрывала и входную строку, и выходную.\n",
        "Воспользуемся высокоуровневыми методами создания обучающей выборки. Для этого входной массив преобразуем в Dataset с помощью `tf.data.Dataset.from_tensor_slices`. Раньше мы имели список символов, а теперь имеем специальный объект-тензор, где внутри тоже список символов. Чтобы достать данные из тензора надо прочитать правила работы с датасетами: https://www.tensorflow.org/guide/data. Там приводится метод enumerate, которым мы воспользуемся как обычным enumerate в Питоне. Другие методы не заработали, так как Dataset создан для работы с потоками данных и не имеет самих данных, чтобы о них что-то рассказать. Не забудем получающиеся тензоры конвертировать в числа с помощью метода numpy() и затем переводить в буквы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHJDA39zf-O"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "print(\"There will be {} examples\".format(examples_per_epoch))\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "print(\"Let's check tensor properties: {}\".format(char_dataset))\n",
        "\n",
        "# Enumerate method\n",
        "for i,v in char_dataset.enumerate().take(7):\n",
        "    print (idx2char[v.numpy()])\n",
        "# Simplier method\n",
        "for v in char_dataset.take(7):\n",
        "    print (idx2char[v.numpy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "Теперь единый тензор разобьём на набор тензоров с точками разбиения на позиции `seq_length+1`. Отбросим хвост текста - не жалко, зато данные выровнены. Опять же пока мы не попытаемся перебрать элементы тензора, тензор о своих данных  ничего не сообщит, хотя теперь он знает одну из размерностей - `seq_length+1`, ведь мы бьём текст именно на такие отрезки.\n",
        "Проверим, что первый элемент это строка, начинающаяся с начала текста, и что нет пропущенных мест при разбиении на подстрочки. Для этого воспользуемся тем, что наш словарь для перевода текста в индексы и обратно может принимать на вход массив и возвращать массив. Поэтому каждая строка это массив символов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4hkDU3i7ozi"
      },
      "source": [
        "# Slicing dataset\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Now we can check the properties\n",
        "print(\"Let's check tensor properties: {}\".format(sequences))\n",
        "\n",
        "# Visualize first 2 sequences translating them to letters\n",
        "for item in sequences.take(2):\n",
        "    print (idx2char[item.numpy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH2lu78AMdy-"
      },
      "source": [
        "Здесь мы перебрали элементы тензора и для каждого сконвертировали его в массив numpy, который надо было перевести в символы. Однако так читать текст неудобно, поэтому можно дополнительно соединить символы методом `join`. Сделаем функцию для вывода строки из тензора с индексами символов и проверим эту функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtAXw6g0Mdy-"
      },
      "source": [
        "def show_str(item, prefix=None):\n",
        "    if prefix is not None:\n",
        "       print(prefix)\n",
        "    print(\"\".join(idx2char[item]))\n",
        "\n",
        "# Visualize first 3 sequences\n",
        "for i,v in sequences.enumerate().take(3):\n",
        "    show_str(v, \"Sequence {}:\".format(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H45M9O4oMdzA"
      },
      "source": [
        "Теперь видно еще лучше, что текст разбит на строки, где каждая следующая продолжает предыдущую."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "Для обучения нам нужны пары строк, сдвинутых на один символ, а не просто много строк из текста. Для этого есть метод map(), который каждый входной элемент массива-тензора заменит на выходной элемент, который ему вернёт некая функция. Эту функцию `split_input_target()` пишем мы сами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "# Define splitting method\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Test splitting method\n",
        "print(split_input_target(\"Test phrase\"))\n",
        "\n",
        "# Creating dataset from sequences with our splitting method\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCopyGZymwi"
      },
      "source": [
        "Проверим что получилось в нашем датасете с помощью уже знакомых методов перебора элементов тензора. Посмотрим первые 2 пары пар строк для обучения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNbw-iR0ymwj"
      },
      "source": [
        "for input_example, target_example in dataset.take(2):\n",
        "    show_str (input_example, 'Input data: ')\n",
        "    show_str (target_example, 'Target data: ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_33OHL3b84i0"
      },
      "source": [
        "Каждая пара строк в обучающей выборке подаётся посимвольно в нейросеть. Каждый шаг по времени t подаётся символ c индексом t входной строки и ожидается на выходе символ c индексом t выходной строки.\n",
        "Для этого объединим строки в кортеж. В строках возьмём только первые 5 символов. Кортеж сделаем с возможностью итерации с помощью `zip()`. Теперь можно способом `enumerate()` перебрать все пары символов (которые на самом деле индексы в словаре символов). Видно, что для каждого символа мы ждём следующий символ на выходе. И так `seq_length` раз, но мы визуализируем только первые 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eBu9WZG84i0"
      },
      "source": [
        "# Take first dataset string pair again\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    pass\n",
        "# Show symbols for each time step as illustration\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, idx2char[input_idx]))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, idx2char[target_idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Создание обучающих пакетов\n",
        "\n",
        "Пары строк в нашем датасете мы разобьём на пакеты (batch) для ускорения обучения. То есть вместо подачи каждой пары строк пока эти пары не кончатся и не пройдёт эпоха, мы будем подавать сразу пачку строк и после каждой пачки подстраивать сеть. Возникает вопрос, какой должен быть размер батча? Крайние случаи это батч из одного элемента и батч из всех элементов. Подстраивать сеть каждую пару строк будет приводить к метаниям градиента, ведь в каждой конретной паре строк свои правила предсказания буквы. Если всё подавать единым батчем, то может не хватить памяти в ускорителе, да и подстраивать коэффициенты раз за эпоху потребует много эпох. Компромисс это сделать количество итераций и зармер батча примерно равными. Мы уже смотрели какой длины у нас датасет (`examples_per_epoch`). Корень из него примерно равен 64. Это и будет размер батча.\n",
        "\n",
        "Для создания батчей используется тот же самый способ, что и при разбиении текста на строки. Тогда от одномерного тензора мы перещли к двумерному (потом мы еще каждый его элемент сделали парой строк), а теперь перейдём к трёхмерному тензору. Это будут пары двумерных массивом чисел, где по первому измерению будет индекс строки, а по второму измерению идут числа, которые являются индексом символа в нашем словаре. \n",
        "Также сейчас пары строк идут одна за одной, а лучше их перемешать. Для этого тоже есть готовые методы объекта tf.data.Dataset.\n",
        "Еще надо будет выбрать размер буфера для перемешивания датасета на ходу. Подробней можно почитать в документации к классу tf.data.Dataset и его методу shuffle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S"
      },
      "source": [
        "# Check dataset classes\n",
        "print(\"dataset class is {}\".format(dataset))\n",
        "print(\"dataset is a child of tf.data.Dataset? {}.\".format(isinstance(dataset, tf.data.Dataset)))\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset_batched = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(\"dataset_batched is new variable of class {}.\".format(dataset_batched))\n",
        "print(\"dataset_batched is a child of tf.data.Dataset? {}.\".format(isinstance(dataset_batched, tf.data.Dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhhWlHyLMdzN"
      },
      "source": [
        "Попробуем снова визуализировать наши символы в датасете из пар массивов строк для входа и выхода."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_2tbv45MdzN"
      },
      "source": [
        "# Take first dataset batch as a pair\n",
        "for input_batch, target_batch in dataset_batched.take(1):\n",
        "    pass\n",
        "\n",
        "# You can check that we have 2D array with the following debug output\n",
        "print(\"input_batch is new variable of class {}.\".format(input_batch))\n",
        "print(\"So dataset_batched can be converted to a pair of arrays with shape {} and {}.\".format(input_batch.shape, target_batch.shape))\n",
        "\n",
        "# Let's look into some strings\n",
        "for i in range(2):\n",
        "    input_example = input_batch[i]\n",
        "    target_example = target_batch[i]\n",
        "    print(\"String pair #{}\".format(i))\n",
        "    show_str(input_example, \"Input sequence:\")\n",
        "    show_str(target_example, \"Target sequence:\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bGmNuxMdzP"
      },
      "source": [
        "Каждый раз, когда мы перезапускаем предыдущий блок, мы получаем разные строки. Это отложенные вычисления. Наш `dataset_batched` не хранит массивы строк в случайном порядке. Он каждый раз порождает по 64 строки длиной 100 символов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Построим модель сети\n",
        "\n",
        "Снова будем использовать модель Sequential `tf.keras.Sequential`. В ней сделаем 3 ключевых слоя. Первый слой нужен для конвертации индекса символа по нашему словарю в более естественное представление для нейронной сети. Например 35 и 36 это очень похожие 2 сигнала, но совершенно разные символы. Используем для такой конвертации one-hot encoding. Встроенного слоя в tensorflow нет, поэтому сделаем свой через Lambda слой. Средним слоев сделаем LSTM (или можно использовать другой RNN нейрон - GRU). Это сделает сеть рекуррентной и позволит сети работать с последовательностями наших объектов: символов-цифр. Последним слоем сделаем обычный Dense с количеством выходов, равным количеству символов в нашем словаре - `vocab_size`, а также привычная активация softmax. Это позволит несложно иметь вероятности следующего символа."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uESreHcMMdzQ"
      },
      "source": [
        "Проверим как работает `tf.one_hot`. На входе должен быть индекс, а на выходе массив с нулями и единицей в месте этого индекса. Мы подадим на вход сразу батч, который двухмерный тензор. Значит на выходе получим трёхмерный тензор."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhoIrFjMMdzQ"
      },
      "source": [
        "for input_batch, target_batch in dataset_batched.take(1):\n",
        "    pass\n",
        "tf.one_hot(input_batch, len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxPZakKOMdzS"
      },
      "source": [
        "Теперь зададим параметры модели в одном месте и саму модель. Только заранее предусмотрим, что модель, которая принимает батч и модель, которая принимает одну строку это разные модели. А мы хотим учиться по батчу, а потом использовать (inference) с одной строкой. Поэтому у нас будут 2 модели, отличающиеся размером батча. Веса у них будут теми же самыми, меняется только размер входного тензора. Поэтому мы будем создавать наши слегка отличающиеся модели через функцию. Ну и сразу зададим глобальные параметры."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtCrdfzEI2N0"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Number of RNN units\n",
        "lstm_units = 1024\n",
        "\n",
        "def build_model(vocab_size, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    # To call tf.one_hot() we use Lambda, but we also have to cast type to uint8 \n",
        "    # because otherwise model weights can't be imported as they do not support default float32 type.\n",
        "    # We also have to define batch shape because model can't be used wighout knowing its input shape.\n",
        "    # It can be defined on first use, but it is more simple to define it now.\n",
        "    tf.keras.layers.Lambda(lambda x: tf.nn.embedding_lookup(tf.cast(x, 'uint8'), vocab_size), \n",
        "                           batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsrpOik5zhv"
      },
      "source": [
        "model_batched = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  rnn_units=lstm_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "model_batched.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "Модель, принимающая батчи, создана и уже может быть использована. Естественно, что мы будем ожидать от неё случайные символы, но мы сможем проверить это и посчитать функцию потерь. Видно, что у модели довольно много коэффициентов, а также, что она готова принимать строки любой длины, но самих строк должно быть `BATCH_SIZE`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Запустим модель без обучения\n",
        "\n",
        "Возьмём один элемент нашего датасета (он каждый раз случайный, мы это обсуждали выше). В нём батч из входных и выходных строк. Посмотрим их размерности, а также заставим сеть по входной строке сгенерировать выходную и посмотрим её размерности."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_70kKAPrPU"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset_batched.take(1):\n",
        "  print(\"Input shape \", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  print(\"Target shape \", target_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  example_batch_predictions = model_batched(input_example_batch)\n",
        "  print(\"Prediction shape \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mVpJQIeMdzZ"
      },
      "source": [
        "Сеть выдаёт батч из строк той же длины, что и входные строки, но вместо символов у нас массивы чисел с длиной `vocab_size`. Всё верно. Нужно выбрать из них символ, который сеть считает следующим. Используем для этого argmax. Возьмём нулевую строку из батча и укажем, что Argmax надо брать вдоль последнего измерения. В итоге мы должны получить строку из 100 индексов массива."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8juMUMCMdzZ"
      },
      "source": [
        "sampled_indices = tf.argmax(example_batch_predictions[0], axis=1)\n",
        "print(sampled_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "Это строка индексов нашего словаря, которая была сгенерирована посимвольно. Сначала мы подали всего один символ и считали символ на выходе. Поэтому даже обученная сеть на данном этапе видела только один символ. Обученная сеть на первый символ \"I\" наверное бы догадалась поставить пробел, так как это наиболее частая последовательность. Но у нас сеть необученная, поэтому она будет генерировать случайные символы.\n",
        "\n",
        "Можно будет перезапустить этот блок после обучения и посмотреть, как сеть пытается генерировать символы строки. Строка на входе и строка на выходе должны быть похожи. При этом важно понимать, что каждый символ предсказанной строки продолжает исходную входную строку, а не выходную. Удачные предсказания это совпадение символов на одинаковых позициях target и predicted строк. После долгого обучения сеть начинает где-то с 10 буквы вспоминать предложение и выдавать его в точности равной target строке."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFmaa6Y2Mdzd"
      },
      "source": [
        "show_str(input_example_batch[0], \"Input sequence:\")\n",
        "show_str(target_example_batch[0], \"Target sequence:\")\n",
        "show_str(sampled_indices, \"Predicted sequence:\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Обучение сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "Итак, мы решаем стандартную проблему классификации. Для этого используем стандартную функцию потерь `tf.keras.losses.sparse_categorical_crossentropy` (она будет применяться к последней размерности выходного тензора) и стандартный оптимизатор `adam`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model_batched.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Проверка перед обучением\n",
        "Мы можем посчитать потери вручную для необученной сети. С этого значения начнётся обучение. Если сеть уже была обучена, то запуск этого блока покажет потери, на которых обучение закончилось и с которых его можно продолжить."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HrXTACTdzY-"
      },
      "source": [
        "example_batch_loss  = tf.keras.losses.sparse_categorical_crossentropy(target_example_batch, example_batch_predictions)\n",
        "print(\"scalar_loss: \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Запуск обучения\n",
        "Введём количество эпох для обучения и запустим его. Обучение 10 эпох занимает разумное время для получения какого-то результата. Для получения хорошего результата лучше учиться 30 и больше эпох."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS=30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-hmKjYVoll"
      },
      "source": [
        "history = model_batched.fit(dataset_batched, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Генерация текста (inference)\n",
        "Пришло время взять обученную сеть и применить её для генерации. Для этого нужно будет решить 2 проблемы: переход к сети с размером батча 1, а не `BATCH_SIZE`; а также использование сети именно для генерации текста, а не продолжении его на одну букву. Последнее мы сделаем рекурсивно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### Переход к размеру батча 1\n",
        "Напомним, что если обучение плохо делать по одной строке, то использование по одной строке делать удобно. И для перехода не нужно переобучать сеть. Нужно создать такую же сеть, только с другим батчем, а потом воспользоваться сохранением и загрузкой весов, как описано в https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/\n",
        "\n",
        "Перед использованием модель надо построить или скомпилировать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKDIPKtjMdzr"
      },
      "source": [
        "model_weights = model_batched.get_weights()\n",
        "\n",
        "model_single = build_model(vocab_size, lstm_units, batch_size=1)\n",
        "\n",
        "model_single.set_weights(model_weights)\n",
        "\n",
        "model_single.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BT34y9UMdzt"
      },
      "source": [
        "Проверим, что модель изменилась и принимает 1 строку."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71xa6jnYVrAN"
      },
      "source": [
        "model_single.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### Рекурсивная генерация\n",
        "\n",
        "Начинаем со стартовой строки. Она может быть любой длины, но именно её сеть должна продолжать. Поэтому стоит ввести хотя бы несколько слов.\n",
        "\n",
        "Входную строку мы должны преобразовать в индекс словаря, затем дать ей еще одно измерение, чтобы это был батч размера 1. А в генерируемой строке мы должны убрать назад лишнее измерение от батча, взять вектор выхода и выбрать наилучший символ, как argmax и преобразовать из индекса массива назад в символ по словарю."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = np.array([char2idx[s] for s in start_string])\n",
        "  print(\"input_eval original shape is \", input_eval.shape)\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  print(\"input_eval shape changed to \", input_eval.shape)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  # We reset the memory of RNN so it will forget the previous timeline os characters sequence\n",
        "  model.reset_states()\n",
        "    \n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # print(\"Predictions shape before squeezing \", predictions.shape)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      # print(\"Predictions shape after squeezing \", predictions.shape)\n",
        "\n",
        "      # using argmax() to determine next symbol\n",
        "      temperature = 1\n",
        "      matrix = np.reshape(predictions[0], (1, -1))\n",
        "      predicted_id = tf.random.categorical(tf.math.log(matrix), 1)\n",
        "      predicted_id = predicted_id[0]\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktovv0RFhrkn"
      },
      "source": [
        "print(generate_text(model_single, start_string=u\"We are on the verge of \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXsKU7q9Mdz0"
      },
      "source": [
        "Текст генерируется, но зациклился? Причина в том, что самый вероятный символ продолжает какую-то фразу, где может встретиться предыдущая короткая последовательность символов, что снова сделает вероятным этот символ. Например, \"Я хочу сказать, что, это, как его, ну это, как его, ну это, как его...\". При этом, если мы запустим блок проверки, который мы ранее запускали на необученной сети, то он работает сносно. Ему на вход поступает строка из нашего датасета. А там нет зацикливания.\n",
        "\n",
        "Способов разорвать цикл 2: более долгая память (более долгое обучение), следование не всегда самому вероятному символу. Долгая память строит более длинные связные фразы и вероятность цикла снижается. Можешь попробовать поучить сеть еще 10 эпох и длина цикла скорее всего вырастет. Второй способ - следование не всегда самому вероятному символу, а другому из вероятных, позволяет выходить из цикла."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8fytlCbMdz1"
      },
      "source": [
        "### Эксперимент 1\n",
        "Чтобы разорвать цикличность текста нужно выбирать не самый вероятный символ, а один из вероятных. У нас же на выходе вектор вероятностей, так давайте им и воспользуемся. Для этого существует функция `tf.random.categorical`. В документации про неё немного сказано, но можно проверить как она действует."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLc0jbWdMdz1"
      },
      "source": [
        "num_of_samples = 200\n",
        "probabilities = np.array([[0.5, 0.2, 0.2, 0.1]])\n",
        "temperature = 1\n",
        "\n",
        "samples = tf.random.categorical(tf.math.log(probabilities) / temperature, num_of_samples)\n",
        "print(probabilities.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkbS_JQZMdz5"
      },
      "source": [
        "На входе матрица вероятностей (нужна именно матрица), а также сколько индексов нагенерировать согласно этим вероятностям. Также обрати внимание на логарифм. Проверим, что статистика генерации совпадает с вероятностями. Для этого воспользуемся нашим стандартным matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLqdvlBNMdz5"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# the histogram of the data\n",
        "_, bins, _ = plt.hist(samples, bins=len(probabilities[0]), \n",
        "                            density=False, color='lawngreen', rwidth = 0.8)\n",
        "# Probability lines\n",
        "for p in probabilities[0]:\n",
        "    plt.plot(bins, [p*num_of_samples for _ in bins], color='b' )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyPq1ePDMdz7"
      },
      "source": [
        "Количество сгенерированных элементов примерно равняется нужному проценту от общего количества. Если не совсем это видно, то можно поднять количество `num_of_samples`.\n",
        "### Задание\n",
        "Примени выбор элемента согласно выдаваемым сетью вероятностям вместо `argmax` и исправь зацикленность текста. При этом ты можешь регулировать разброс вероятностей с помощью `temperature`. Нулевая температура по сути эквивалентна argmax, а бесконечная температура просто выдаёт случайный символ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVONpOE9Mdz8"
      },
      "source": [
        "### Эксперимент 2\n",
        "Повысим скорость обучения сети и одновременно упростим её за счёт избавления от вручную созданного слоя. Вместо жесткой кодировки каждого символа в one-hot вектор можно использовать эмбеддинги. А именно кодировку индекса в вектор, где сложие объекты имеют сонаправленные значения, а противоположные объекты по смыслу будут противонаправленные. Эмбеддинги в основном используют для слов, а не для символов, но и для символов они подходят. Например пунктуация сложа между собой, а символы от них отличны. Проще всего понять как должны работать эмбеддинги можно через пример. Должно примерно выполняться следующее равенство векторов из эмбеддингов слов \"Король\" - \"Мужчина\" + \"Женщина\" ~= \"Королева\".\n",
        "### Задание\n",
        "Примени слой эмбеддингов вместо слоя one-hot для символов, найдя его в документации и подобрав размерность эмбеддинга. Запиши насколько упали потери после 10 эпох обучения по сравнению с прошлой сетью со слоем one-hot. Не меняй сеть обратно, раз с эмбеддингами работает лучше."
      ]
    }
  ]
}