{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Занятие 11 - Обучение с подкреплением (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Новые зависимости\n",
    "Нужно установить gym с помощью pip.\n",
    "Также потребуется ffmpeg. На OS X используем `brew install ffmpeg`. На большинстве Ubuntu `sudo apt-get install ffmpeg`. На Ubuntu 14.04 используем `sudo apt-get install libav-tools`.\n",
    "\n",
    "На Windows всё посложнее. Скачать можно по ссылке: https://www.gyan.dev/ffmpeg/builds/ffmpeg-git-essentials.7z, дальше распаковать, поместить на диске в место, где это дальше будет использоваться. То есть в C:\\ffmpeg или в C:\\Program Files\\ffmpeg. Внутри этой папки должен лежать bin, doc и т.д. Далее нужно пойти в настройку переменных окружения, где задать системный путь (Path). Не путать с пользовательским путём. И вписать в одно из полей пути C:\\Program Files\\ffmpeg\\bin (если выбрали второй вариант). Ну или аналогично. Затем перелогиниться в систему или перезагрузиться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением как обучение без учителя\n",
    "\n",
    "### Обучение с учителем\n",
    "Если у нас есть набор примеров с правильными ответами, то мы используем эту выборку для обучения нашей модели, а после обучения, применяем её к неразмеченным данным. Именно этот подход мы использовали, когда обучали классификатор для MNIST, подавая на вход сети картинки с изображениями рукописных цифр и считая градиент для подстройки весов на основе разницы между известным лэйблом цифры и выходом нейросети.\n",
    "### Обучение без учителя\n",
    "В некоторых случаях у нас нет размеченных данных, на которых мы могли бы заранее обучить модель. Но, при решении некоторых задач, можно обойтись без размеченной выборки. Примером такой задачи является задача кластеризации.\n",
    "### Обучение с подкреплением\n",
    "В некоторых случаях существующие методы обучения без учителя нам не подходят. В то же время у нас нет возможности создать качественную обучающую выборку. При этом мы можем постфактум оценить действия нашей модели и использовать эту оценку подстроить модель так, чтобы она чаще совершала желательные действия и реже - нежелательные. В литературе такую оценку называют вознаграждением (reward), а обучение строится таким образом, чтобы это модель стремилась максимизировать получаемое вознаграждение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наиболее яркие примеры использования обучения с подкреплением\n",
    "- В 2013 году DeepMind публикует статью Playing Atari with Deep Reinforcement Learning, где нейросети обучаются игре в старые игры от Atari, используя анализ изображения.\n",
    "- В 2016 году нейросеть AlphaGO Google DeepMind  обыгрывает одного из сильнейших игроков в Go - Ли Седоля. При обучении AlphaGo использовались партии игры живых людей. Чуть позже будет представлена AlphaGO Zero, обучение которой было полностью построено на игре с самой собой. Новая сеть выиграла у старой со счётом 100:0, причём аппаратные ресурсы сократились с 48 TPU до 4 TPU (Tensor Processing Unit Google).\n",
    "- В 2017 году нейросеть OpenAI 5 успешно участвует в соревновании по игре Dota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Терминология: агент, функция награды, состояние среды\n",
    "\n",
    "Агент и среда - ключевые понятия в обучении с подкрелением.\n",
    "\n",
    "**Агент** - программа, принимающая решение о дальнейших действиях на основе информации о состоянии среды.\n",
    "\n",
    "**Среда** - это мир, в котором агент должен \"выживать\", т.е. всё, с чем агент может прямо или косвенно взаимодействовать. Среда обладает состоянием (State), агент может влиять на среду, совершая какие-то действия (Actions), переводя среду при этом из одного состояния в другое и получая какое-то вознаграждение. Среда описывается пространством возможных состояний. Конкретное состояние - вектор в этом пространстве.\n",
    "\n",
    "<img src=\"agend_and_environment.gif\">\n",
    "\n",
    "В зависимости от конкретной задачи, агент может наблюдать либо полное состояние среды, либо только некоторую его часть. Во втором случае, агенту может потребоваться какое-то внутреннее представление полного состояния, которое будет обновляться по мере получения новых данных.\n",
    "**Фукнция награды** - вводимая программистом формула вычисления ценности действия на основе финального результата, ожидания этого результата, промежуточных результатов и любых других параметров, которые будут подсказывать путь к наилучшей последовательности действий агента. Это некоторый аналог функции потерь, без которой непонятно чему учиться. Например, в шахматах истинная награда это победа, но взятая фигура соперника тоже ценна и должна увеличивать награду, если мы хотим подсказать агенту, что брать чужие фигуры полезно. Может ли после этого агент получать мат, позарившись на незащищенную фигуру? Да, ровно как и неопытный шахматный игрок. Попытка передать через дополнительные неосновные награды подсказки к получению основной награды называется **reward shaping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация алгоритмов RL\n",
    "### Model-free / Model-based\n",
    "Model-free не строит модель окружения или функции награды. Это проще. Model-based алгоритм пытается предсказывать, каким будет следующее состояние окружения или вознаграждение. Это позволяет мыслить на несколько шагов вперёд. Например, совершенно не больно прыгать с крыши. Больно потом, когда разбиваешься о землю. Модель среды и награды позволяет принимать решения не прыгать с крыши, хотя и model-free подход позволяет это понять, хоть и более сложно и грубо.\n",
    "### Value-based / policy-based\n",
    "Policy-based  методы оптимизируют напрямую функцию принятия решения агента. Стратегия (policy) обычно представлена распределением вероятности доступных действий. Value-based метод оптимизирует оценку вознаграждения для всех действий и выбирает выбирает то действие, по которому прогнозируется большее значение. Методы, основанные на Policy Gradients лучше работают при большой размерности пространства действий, а Value-based методы, такие, как Deep Q-Learning требуют меньшего количества повторений для сходимости при малой размерности.\n",
    "### On-Policy / Off-Policy\n",
    "Off-policy подход позволяет учиться на исторических данных или на записанных заранее действиях человека. On-policy - только на собственных действиях агента. Это довольно важное деление, так как обучение на собственных действиях просто недоступно для многих задач. Плохо ли автомобилю с автопилотом въехать в стену? Надо попробовать, так будет работать on-policy метод. А лучше взять понимание, что это плохо из готовых данных, синтетических, например и усвоить этот опыт. Даже если мы используем симуляцию среды, а не реальную среду, то количество эпизодов симуляции обычно порядка сотен тысяч и их симуляция съедает много времени. Каждый раз проходить переобучение с нуля очень неудобно, а возможность импортировать накопленный опыт симуляции среды сильно экономит ресурсы. Однако off-policy методы можно применить не всегда.\n",
    "### Deterministic Policy / Stochastic Policy\n",
    "В зависимости от среды, наша стратегия может быть либо детерминированной - выбираем сразу определённое действие с помощью argmax, либо стохастической, когда мы окончательное решение принимается с помощью генератора случайных чисел на основе распределения вероятности, выданного сетью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типовые ошибки при обучении с подкреплением и их причины\n",
    "\n",
    "### Низкая скорость обучения (sample efficiency)\n",
    "\n",
    "Общая проблема всех алгоритмов обучения с подкреплением - низкая скорость обучения. В то время, как человеку может быть достаточно одного повторения, чтобы выучить какое-то действие, агенту RL требуется десятки тысяч повторений даже в простых задачах. В какой-то степени это связано с несовершенством архитектуры, но самый большой вклад даёт тот факт, что человек может использовать накопленный в прошлом опыт из других областей. Игра Montezuma's Revenge - популярный подопытная среда для RL в последнее время. И яркий пример низкой эффективности повторений  у алгоритмов RL по сравнению с человеком. \n",
    "\n",
    "Челокек, как правило, быстро понимает, что нужно избегать черепа и забрать ключ, гравитация направлена вниз, а падение с большой высоты опасно. Алгоритму же приходиться обучаться с полного нуля. Если же подменить элементы интерфейса на неочевидные для человека, то его sample-efficency тоже сильно падает (хотя всё-равно лучше, чем RL).\n",
    "\n",
    "<img src=\"game_prior.gif\" width=\"700\">\n",
    "\n",
    "А теперь то же самое, но в нечеловекочитаемом виде. Для RL разницы нет, а для человека сразу стало сложнее.\n",
    "\n",
    "<img src=\"game_no_prior.gif\" width=\"700\">\n",
    "\n",
    "### Сложное проектирование функции награды\n",
    "Так же важным фактором являются редкие награды. Часто в ходе одного эпизода алгоритм делает множество различных действий, а награду полуает только в конце. Соответственно, веса сети можно обновить только в конце эпизода и нельзя поощрить или наказать конкретные действия внутри эпизода. В итоге требуется большое количество повторений для достижения оптимальных весов.\n",
    "\n",
    "Один из способов улучшить эффективность при редких наградах - reward shaping - модификация функции награды так, чтобы явно поощрялись какие-то действия внутри эпизода. Но качественно сконструировать такую функцию тяжело, а ошибки в ней могут приводить к неожиданным эффектам:\n",
    "\n",
    "<img src=\"coastrunner.gif\" width=\"700\">\n",
    "\n",
    "В гонке лодок агент получал вознаграждене не только за победу в гонке, но и за сбор игровых бонусов. В итоге он решил, что гонка не очень-то и нужна, достаточно собирать бонусы.\n",
    "\n",
    "<video controls src=\"upsidedown_half_cheetah.mp4\" width=\"700\"> </video>\n",
    "\n",
    "У данного агента мы наблюдаем поподание в локальный минимум. Этот агент получает поощрение за набранную скорость. На начальном этапе во время случайного поиска агент обнаружил, что кувыркнуться вперёд даёт хорошее вознаграждение в начале. Постепенно, после нескольких попыток, переворачивание на спину закрепилось, как успешная стратегия. После закрепления такого поведения агент не смог выйти из этого состояния, т.к. оказалось проще научиться двигаться в таком состоянии, чем научиться переворачиваться обратно на ноги.\n",
    "\n",
    "Похожее поведение можно случано получить, если поощрять агента за то, что его ноги оторваны от земли.\n",
    "    \n",
    "<video controls src=\"failed_reacher.mp4\" width=\"700\"> </video>\n",
    "\n",
    "В данном примере случайная инициализация весов получилась такой, что к вращающейся \"конечности\" в каждой точке прикладывалась большая сила. В результате конечность начала быстро вращаться. Сложность избавления от такого поведения заключается в том, что для того, чтобы отступить от такой стратегии, нужно путём исследования случайных действий предпринять несколько попыток, когда робот не будет вращаться, чтобы такие действия могли закрепиться. Это возможно, но в данном запуске этого не произошло.\n",
    "### Невоспроизводимость обучения\n",
    "Обычно мы всегда начинаем со случайного распределения весов, что не мешает стабильно получать результат обучения. Но с RL это не так. Даже в простой задаче с 3 степенями свободы (state - трёхмерный вектор) и одной степенью воздействия (action - скаляр), обучение с разными зародышами генератора псевдослучайных чисел привело в 30% случаев к фиаско. Заранее понять, что обучение пошло плохо, нельзя или сложно.\n",
    "<img src=\"pendulum_results.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека OpenAI Gym\n",
    "В OpenAI создали готовую библиотеку для моделировавния, визуализирования и обучения, что не только удобно для тестов. Одно из хороших преимуществ это качественный абстрактный интерфейс взаимодействия среды и агента, который позволяет делать совместимые с Gym среды и впоследствии интегрироваться в экосистему, легко подменять типы агентов, тестировать, сравнивать.\n",
    "Среди готовых сред для взаимодействия с ними есть\n",
    "- игры Atari - полноценные компьютерные игры, типа Арканоида, где вместо человека в компьютер играет агент.\n",
    "- классические настольные пошаговые игры: шахматы, go.\n",
    "- физические симуляции, где нужно управлять физическим объектом: маятник, который должен перевернуться и балансировать за счёт раскачивания; тележка, которая должна проехать через холм за счёт раскачивания.\n",
    "- и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример окружения Gym\n",
    "В этом примере загружается готовая среда: машинка должна заехать на горку. Функция награды встроенная. А агент - случайное воздействие. Оно ничему не учится, лишь хаотически выдаёт действия, но это показывает где взять все необходимые данные для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02551045  0.0300331   0.02239181 -0.01126388]\n",
      "[ 0.02611111 -0.16540271  0.02216653  0.28839898]\n",
      "[0.02280305 0.02939624 0.02793451 0.00278877]\n",
      "[ 0.02339098  0.22410667  0.02799029 -0.28095124]\n",
      "[0.02787311 0.02859686 0.02237126 0.02042666]\n",
      "[ 0.02844505  0.22339096  0.0227798  -0.26511477]\n",
      "[0.03291287 0.02795141 0.0174775  0.03466518]\n",
      "[ 0.0334719  -0.16741676  0.0181708   0.3328108 ]\n",
      "[ 0.03012356 -0.36279256  0.02482702  0.63116805]\n",
      "[ 0.02286771 -0.55825196  0.03745038  0.93156519]\n",
      "[ 0.01170267 -0.36365486  0.05608169  0.65088205]\n",
      "[ 0.00442957 -0.55951123  0.06909933  0.9606841 ]\n",
      "[-0.00676065 -0.36538266  0.08831301  0.69048518]\n",
      "[-0.0140683  -0.56161186  0.10212271  1.00961219]\n",
      "[-0.02530054 -0.36799019  0.12231496  0.75066477]\n",
      "[-0.03266035 -0.17474839  0.13732825  0.49883866]\n",
      "[-0.03615531 -0.37151211  0.14730502  0.83145325]\n",
      "[-0.04358556 -0.56830701  0.16393409  1.16659996]\n",
      "[-0.0549517  -0.37565298  0.18726609  0.92947436]\n",
      "[-0.06246475 -0.1834848   0.20585558  0.70099664]\n",
      "Episode finished after 20 timesteps\n",
      "[-0.02764377 -0.03059532  0.02776475 -0.01726595]\n",
      "[-0.02825568 -0.22610422  0.02741944  0.28404616]\n",
      "[-0.03277777 -0.03138385  0.03310036  0.00013571]\n",
      "[-0.03340544  0.16324814  0.03310307 -0.28192263]\n",
      "[-0.03014048  0.35788266  0.02746462 -0.56398393]\n",
      "[-0.02298283  0.5526087   0.01618494 -0.84788926]\n",
      "[-1.19306526e-02  7.47506191e-01 -7.72842891e-04 -1.13543904e+00]\n",
      "[ 0.00301947  0.94263825 -0.02348162 -1.42836425]\n",
      "[ 0.02187224  0.74781406 -0.05204891 -1.14311149]\n",
      "[ 0.03682852  0.55340939 -0.07491114 -0.86719487]\n",
      "[ 0.04789671  0.7494664  -0.09225504 -1.1824589 ]\n",
      "[ 0.06288603  0.94565643 -0.11590421 -1.50257702]\n",
      "[ 0.08179916  0.75211653 -0.14595575 -1.24821457]\n",
      "[ 0.09684149  0.94877665 -0.17092005 -1.58282703]\n",
      "[ 0.11581703  1.14547028 -0.20257659 -1.92357452]\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.00361077 -0.033655   -0.04935926 -0.01811852]\n",
      "[ 0.00293767  0.1621388  -0.04972163 -0.32595725]\n",
      "[ 0.00618044  0.35793211 -0.05624077 -0.63389639]\n",
      "[ 0.01333908  0.16363792 -0.0689187  -0.35944197]\n",
      "[ 0.01661184 -0.0304401  -0.07610754 -0.08926211]\n",
      "[ 0.01600304  0.16568558 -0.07789278 -0.40495304]\n",
      "[ 0.01931675  0.36182077 -0.08599184 -0.72114109]\n",
      "[ 0.02655317  0.55802043 -0.10041466 -1.03960359]\n",
      "[ 0.03771358  0.75432277 -0.12120673 -1.36204617]\n",
      "[ 0.05280003  0.95073708 -0.14844766 -1.69005373]\n",
      "[ 0.07181477  0.75760992 -0.18224873 -1.44703254]\n",
      "Episode finished after 11 timesteps\n",
      "[-0.01346905  0.02489376  0.0109381  -0.00573821]\n",
      "[-0.01297117  0.21985715  0.01082334 -0.29495006]\n",
      "[-0.00857403  0.41482314  0.00492434 -0.58419992]\n",
      "[-2.77566978e-04  6.09875760e-01 -6.75966183e-03 -8.75327558e-01]\n",
      "[ 0.01191995  0.80508895 -0.02426621 -1.17012797]\n",
      "[ 0.02802173  0.61029085 -0.04766877 -0.88515042]\n",
      "[ 0.04022754  0.80602646 -0.06537178 -1.19242964]\n",
      "[ 0.05634807  1.00193148 -0.08922037 -1.50486511]\n",
      "[ 0.0763867   0.8079981  -0.11931768 -1.2413181 ]\n",
      "[ 0.09254667  1.00443245 -0.14414404 -1.56887089]\n",
      "[ 0.11263531  1.20095147 -0.17552146 -1.90282409]\n",
      "Episode finished after 11 timesteps\n",
      "[-0.00393341  0.03479933 -0.02299709 -0.04143692]\n",
      "[-0.00323742 -0.15998543 -0.02382582  0.24390241]\n",
      "[-0.00643713  0.03546859 -0.01894778 -0.05619954]\n",
      "[-0.00572776 -0.15937663 -0.02007177  0.23044551]\n",
      "[-0.00891529 -0.35420609 -0.01546286  0.51673014]\n",
      "[-0.01599941 -0.54910694 -0.00512825  0.80450056]\n",
      "[-0.02698155 -0.74415821  0.01096176  1.09556592]\n",
      "[-0.04186472 -0.9394228   0.03287308  1.3916679 ]\n",
      "[-0.06065317 -0.74472524  0.06070643  1.1094423 ]\n",
      "[-0.07554768 -0.55045124  0.08289528  0.83640483]\n",
      "[-0.0865567  -0.74660168  0.09962338  1.15396347]\n",
      "[-0.10148873 -0.55291003  0.12270265  0.89410741]\n",
      "[-0.11254694 -0.74946319  0.1405848   1.2227046 ]\n",
      "[-0.1275362  -0.55640396  0.16503889  0.97716868]\n",
      "[-0.13866428 -0.75330813  0.18458226  1.31681309]\n",
      "Episode finished after 15 timesteps\n",
      "[-0.04104536  0.0416703   0.01986987  0.02767789]\n",
      "[-0.04021196 -0.15373089  0.02042342  0.32656316]\n",
      "[-0.04328657  0.04109443  0.02695469  0.04039021]\n",
      "[-0.04246468  0.23581969  0.02776249 -0.24366786]\n",
      "[-0.03774829  0.43053432  0.02288913 -0.52746613]\n",
      "[-0.0291376   0.62532687  0.01233981 -0.8128496 ]\n",
      "[-0.01663107  0.82027765 -0.00391718 -1.10162569]\n",
      "[-2.25514527e-04  6.25207454e-01 -2.59496939e-02 -8.10174295e-01]\n",
      "[ 0.01227863  0.82067515 -0.04215318 -1.11090548]\n",
      "[ 0.02869214  0.62613156 -0.06437129 -0.83173869]\n",
      "[ 0.04121477  0.8220714  -0.08100606 -1.14395182]\n",
      "[ 0.0576562   1.01815277 -0.1038851  -1.46089914]\n",
      "[ 0.07801925  1.21438358 -0.13310308 -1.78414577]\n",
      "[ 0.10230692  1.41072679 -0.168786   -2.11507243]\n",
      "Episode finished after 14 timesteps\n",
      "[-0.02488668 -0.00061576  0.01232854  0.00808184]\n",
      "[-0.024899    0.19432723  0.01249017 -0.28068593]\n",
      "[-0.02101245  0.38926881  0.00687646 -0.56940347]\n",
      "[-0.01322708  0.58429365 -0.00451161 -0.85991215]\n",
      "[-0.0015412   0.38923343 -0.02170986 -0.56865122]\n",
      "[ 0.00624346  0.58465305 -0.03308288 -0.86809376]\n",
      "[ 0.01793653  0.78020914 -0.05044476 -1.17099198]\n",
      "[ 0.03354071  0.97594945 -0.0738646  -1.47905342]\n",
      "[ 0.0530597   0.78180284 -0.10344566 -1.21032403]\n",
      "[ 0.06869575  0.97809706 -0.12765215 -1.53355018]\n",
      "[ 0.0882577   0.78472323 -0.15832315 -1.28327739]\n",
      "[ 0.10395216  0.59193146 -0.1839887  -1.04405938]\n",
      "[ 0.11579079  0.39966535 -0.20486988 -0.81431589]\n",
      "Episode finished after 13 timesteps\n",
      "[ 0.04449362 -0.01110924  0.00547692  0.01419256]\n",
      "[ 0.04427144  0.18393374  0.00576077 -0.27675731]\n",
      "[ 0.04795011 -0.01126993  0.00022562  0.01773698]\n",
      "[ 0.04772472  0.18384879  0.00058036 -0.27487475]\n",
      "[ 0.05140169 -0.01128144 -0.00491713  0.01799117]\n",
      "[ 0.05117606 -0.20633253 -0.00455731  0.30911864]\n",
      "[ 0.04704941 -0.01114594  0.00162506  0.01500195]\n",
      "[ 0.04682649 -0.20629116  0.0019251   0.30819716]\n",
      "[ 0.04270067 -0.01119669  0.00808905  0.01612198]\n",
      "[ 0.04247674  0.18380832  0.00841149 -0.27399782]\n",
      "[ 0.0461529  -0.01143263  0.00293153  0.02132619]\n",
      "[ 0.04592425 -0.2065965   0.00335805  0.31493261]\n",
      "[ 0.04179232 -0.40176612  0.0096567   0.60867266]\n",
      "[ 0.033757   -0.20678049  0.02183016  0.31904688]\n",
      "[ 0.02962139 -0.01197614  0.0282111   0.03332754]\n",
      "[ 0.02938186  0.18273013  0.02887765 -0.2503227 ]\n",
      "[ 0.03303647  0.37742805  0.02387119 -0.53375897]\n",
      "[ 0.04058503  0.18197866  0.01319601 -0.23365095]\n",
      "[ 0.0442246   0.37690961  0.00852299 -0.52214239]\n",
      "[ 0.05176279  0.18166873 -0.00191985 -0.22678599]\n",
      "[ 0.05539617  0.37681807 -0.00645557 -0.52007389]\n",
      "[ 0.06293253  0.18178759 -0.01685705 -0.22943219]\n",
      "[ 0.06656828  0.37714632 -0.0214457  -0.52738439]\n",
      "[ 0.07411121  0.18233258 -0.03199338 -0.24153552]\n",
      "[ 0.07775786  0.37789658 -0.03682409 -0.54413586]\n",
      "[ 0.08531579  0.57351613 -0.04770681 -0.8481901 ]\n",
      "[ 0.09678611  0.76925519 -0.06467061 -1.15548511]\n",
      "[ 0.11217122  0.5750333  -0.08778031 -0.88376119]\n",
      "[ 0.12367188  0.77123055 -0.10545554 -1.20269826]\n",
      "[ 0.13909649  0.96754601 -0.1295095  -1.52648205]\n",
      "[ 0.15844741  1.1639713  -0.16003915 -1.85662194]\n",
      "[ 0.18172684  1.36044846 -0.19717158 -2.19442244]\n",
      "Episode finished after 32 timesteps\n",
      "[-0.019808   -0.01978198 -0.02471193 -0.04542361]\n",
      "[-0.02020364 -0.21454102 -0.0256204   0.23936118]\n",
      "[-0.02449446 -0.01906261 -0.02083318 -0.06129177]\n",
      "[-0.02487571 -0.21387976 -0.02205901  0.22474603]\n",
      "[-0.0291533  -0.40867961 -0.01756409  0.51038989]\n",
      "[-0.0373269  -0.21331469 -0.00735629  0.21222413]\n",
      "[-0.04159319 -0.01808834 -0.00311181 -0.0827702 ]\n",
      "[-0.04195496  0.17707808 -0.00476721 -0.37643328]\n",
      "[-0.03841339 -0.01797584 -0.01229588 -0.08525729]\n",
      "[-0.03877291  0.17732019 -0.01400103 -0.38179408]\n",
      "[-0.03522651 -0.01760019 -0.02163691 -0.09355834]\n",
      "[-0.03557851  0.17782509 -0.02350807 -0.3929884 ]\n",
      "[-0.03202201  0.37327262 -0.03136784 -0.69298942]\n",
      "[-0.02455656  0.17859955 -0.04522763 -0.4103441 ]\n",
      "[-0.02098457 -0.015853   -0.05343451 -0.13225594]\n",
      "[-0.02130163 -0.21017043 -0.05607963  0.14310192]\n",
      "[-0.02550504 -0.01429205 -0.05321759 -0.16673253]\n",
      "[-0.02579088  0.18154969 -0.05655224 -0.47571784]\n",
      "[-0.02215988  0.37742269 -0.0660666  -0.78567514]\n",
      "[-0.01461143  0.57338712 -0.0817801  -1.09838997]\n",
      "[-0.00314369  0.37943135 -0.1037479  -0.8324452 ]\n",
      "[ 0.00444494  0.57580642 -0.12039681 -1.15587114]\n",
      "[ 0.01596107  0.38244205 -0.14351423 -0.90323542]\n",
      "[ 0.02360991  0.57918575 -0.16157894 -1.23736553]\n",
      "[ 0.03519363  0.38646561 -0.18632625 -0.99934851]\n",
      "[ 0.04292294  0.58352302 -0.20631322 -1.34428195]\n",
      "Episode finished after 26 timesteps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00312564 -0.03000045 -0.01629596  0.03873857]\n",
      "[-0.00372565 -0.22488497 -0.01552119  0.32623574]\n",
      "[-0.00822335 -0.41978254 -0.00899647  0.61398383]\n",
      "[-0.016619   -0.61477763  0.0032832   0.9038197 ]\n",
      "[-0.02891456 -0.80994389  0.0213596   1.19753278]\n",
      "[-0.04511343 -1.00533568  0.04531025  1.49683284]\n",
      "[-0.06522015 -0.81079285  0.07524691  1.21863476]\n",
      "[-0.081436   -0.61671731  0.09961961  0.95044694]\n",
      "[-0.09377035 -0.42306712  0.11862854  0.69065156]\n",
      "[-0.10223169 -0.22977359  0.13244158  0.43754404]\n",
      "[-0.10682716 -0.42649713  0.14119246  0.76886922]\n",
      "[-0.11535711 -0.23357168  0.15656984  0.52373356]\n",
      "[-0.12002854 -0.43051045  0.16704451  0.86137197]\n",
      "[-0.12863875 -0.23800888  0.18427195  0.62552192]\n",
      "[-0.13339893 -0.43515988  0.19678239  0.97011161]\n",
      "Episode finished after 15 timesteps\n",
      "[-0.02274657 -0.02279754  0.01189879 -0.01964412]\n",
      "[-0.02320252  0.17215177  0.01150591 -0.30854922]\n",
      "[-0.01975948  0.3671079   0.00533492 -0.5975814 ]\n",
      "[-0.01241733  0.17191171 -0.00661671 -0.30322282]\n",
      "[-0.00897909 -0.02311532 -0.01268116 -0.01263398]\n",
      "[-0.0094414  -0.21805314 -0.01293384  0.27602107]\n",
      "[-0.01380246 -0.41298819 -0.00741342  0.56459676]\n",
      "[-0.02206223 -0.60800535  0.00387852  0.85493493]\n",
      "[-0.03422233 -0.80317994  0.02097721  1.1488349 ]\n",
      "[-0.05028593 -0.60833801  0.04395391  0.86280315]\n",
      "[-0.06245269 -0.80402996  0.06120997  1.16897585]\n",
      "[-0.07853329 -0.99989238  0.08458949  1.48020378]\n",
      "[-0.09853114 -1.19593869  0.11419357  1.79806145]\n",
      "[-0.12244991 -1.39213835  0.1501548   2.12394415]\n",
      "[-0.15029268 -1.19879582  0.19263368  1.88117342]\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.02718745 -0.03284243  0.02592346  0.04035172]\n",
      "[ 0.0265306  -0.22832635  0.02673049  0.34109972]\n",
      "[ 0.02196407 -0.42381822  0.03355249  0.6420906 ]\n",
      "[ 0.01348771 -0.6193914   0.0463943   0.94514784]\n",
      "[ 0.00109988 -0.42492405  0.06529726  0.66739572]\n",
      "[-0.0073986  -0.62089039  0.07864517  0.97990255]\n",
      "[-0.01981641 -0.81697343  0.09824322  1.29621584]\n",
      "[-0.03615588 -1.01319637  0.12416754  1.61796804]\n",
      "[-0.05641981 -0.81973773  0.1565269   1.36642574]\n",
      "[-0.07281456 -1.01643431  0.18385542  1.70369479]\n",
      "Episode finished after 10 timesteps\n",
      "[ 0.00717155 -0.00401783  0.00929067  0.02235402]\n",
      "[ 0.00709119 -0.19927178  0.00973775  0.31795371]\n",
      "[ 0.00310576 -0.00428987  0.01609682  0.02835756]\n",
      "[ 0.00301996  0.19059759  0.01666397 -0.25920349]\n",
      "[ 0.00683191  0.38547773  0.0114799  -0.54658414]\n",
      "[ 1.45414660e-02  5.80436525e-01  5.48220037e-04 -8.35628041e-01]\n",
      "[ 0.0261502   0.38530709 -0.01616434 -0.54277275]\n",
      "[ 0.03385634  0.58065244 -0.0270198  -0.84050461]\n",
      "[ 0.04546939  0.38590958 -0.04382989 -0.5564398 ]\n",
      "[ 0.05318758  0.1914295  -0.05495868 -0.2778817 ]\n",
      "[ 0.05701617  0.38729067 -0.06051632 -0.58737959]\n",
      "[ 0.06476198  0.5832056  -0.07226391 -0.89849499]\n",
      "[ 0.07642609  0.38913358 -0.09023381 -0.62937381]\n",
      "[ 0.08420877  0.58539117 -0.10282129 -0.9490553 ]\n",
      "[ 0.09591659  0.3917925  -0.12180239 -0.69036724]\n",
      "[ 0.10375244  0.58837521 -0.13560974 -1.01877808]\n",
      "[ 0.11551994  0.39529566 -0.1559853  -0.77156491]\n",
      "[ 0.12342586  0.59218064 -0.1714166  -1.10898073]\n",
      "[ 0.13526947  0.39967437 -0.19359621 -0.87460338]\n",
      "Episode finished after 19 timesteps\n",
      "[-0.02499194 -0.03004808  0.04306235 -0.01835391]\n",
      "[-0.0255929  -0.22576029  0.04269527  0.28759868]\n",
      "[-0.0301081  -0.03127239  0.04844725  0.00868128]\n",
      "[-0.03073355 -0.22705446  0.04862087  0.31624773]\n",
      "[-0.03527464 -0.03265756  0.05494583  0.03928588]\n",
      "[-0.03592779  0.16163518  0.05573155 -0.23556746]\n",
      "[-0.03269509 -0.03423691  0.0510202   0.07416109]\n",
      "[-0.03337983 -0.23005176  0.05250342  0.38249459]\n",
      "[-0.03798086 -0.42587835  0.06015331  0.69125862]\n",
      "[-0.04649843 -0.62178107  0.07397848  1.00225564]\n",
      "[-0.05893405 -0.42772135  0.0940236   0.73369207]\n",
      "[-0.06748848 -0.23401553  0.10869744  0.47201895]\n",
      "[-0.07216879 -0.43049134  0.11813782  0.7968875 ]\n",
      "[-0.08077861 -0.23717142  0.13407557  0.54358061]\n",
      "[-0.08552204 -0.4338975   0.14494718  0.87532162]\n",
      "[-0.09419999 -0.63066087  0.16245361  1.20983706]\n",
      "[-0.10681321 -0.43796585  0.18665035  0.97215179]\n",
      "[-0.11557253 -0.24577216  0.20609339  0.74342561]\n",
      "Episode finished after 18 timesteps\n",
      "[-0.00711736 -0.00809577 -0.01762902  0.03050025]\n",
      "[-0.00727927 -0.20296054 -0.01701902  0.31756932]\n",
      "[-0.01133848 -0.397836   -0.01066763  0.60483683]\n",
      "[-0.0192952  -0.20256651  0.00142911  0.30881303]\n",
      "[-0.02334653 -0.00746495  0.00760537  0.01658115]\n",
      "[-0.02349583 -0.20269514  0.00793699  0.31165392]\n",
      "[-0.02754973 -0.00768716  0.01417007  0.02148461]\n",
      "[-0.02770348  0.18722875  0.01459976 -0.26669403]\n",
      "[-0.0239589   0.38213932  0.00926588 -0.55473655]\n",
      "[-0.01631612  0.57712995 -0.00182885 -0.84448581]\n",
      "[-0.00477352  0.382033   -0.01871857 -0.55237856]\n",
      "[ 0.00286714  0.18717886 -0.02976614 -0.26565149]\n",
      "[ 0.00661072  0.38271273 -0.03507917 -0.56757232]\n",
      "[ 0.01426497  0.18809994 -0.04643061 -0.28614389]\n",
      "[ 0.01802697  0.38385226 -0.05215349 -0.59310162]\n",
      "[ 0.02570402  0.57966398 -0.06401552 -0.90174602]\n",
      "[ 0.0372973   0.77559209 -0.08205044 -1.21384391]\n",
      "[ 0.05280914  0.9716713  -0.10632732 -1.53106901]\n",
      "[ 0.07224257  1.16790224 -0.1369487  -1.85495449]\n",
      "[ 0.09560061  0.9745253  -0.17404779 -1.60774209]\n",
      "[ 0.11509112  0.78183545 -0.20620263 -1.37398619]\n",
      "Episode finished after 21 timesteps\n",
      "[ 0.049902    0.0495366   0.0110344  -0.03458108]\n",
      "[ 0.05089273 -0.14574183  0.01034278  0.26156279]\n",
      "[ 0.0479779   0.04923096  0.01557404 -0.02784004]\n",
      "[ 0.04896252  0.24412615  0.01501724 -0.31556878]\n",
      "[ 0.05384504  0.43903101  0.00870586 -0.6034782 ]\n",
      "[ 0.06262566  0.63403012 -0.0033637  -0.8934063 ]\n",
      "[ 0.07530626  0.43895395 -0.02123183 -0.60178263]\n",
      "[ 0.08408534  0.63436636 -0.03326748 -0.90107687]\n",
      "[ 0.09677267  0.43971057 -0.05128902 -0.61903358]\n",
      "[ 0.10556688  0.2453411  -0.06366969 -0.34293513]\n",
      "[ 0.1104737   0.05118002 -0.07052839 -0.07098987]\n",
      "[ 0.1114973   0.24723851 -0.07194819 -0.38506464]\n",
      "[ 0.11644207  0.44330421 -0.07964949 -0.69953684]\n",
      "[ 0.12530816  0.63943479 -0.09364022 -1.0161922 ]\n",
      "[ 0.13809685  0.44567769 -0.11396407 -0.75431985]\n",
      "[ 0.14701041  0.25229589 -0.12905046 -0.49956177]\n",
      "[ 0.15205632  0.44897846 -0.1390417  -0.82996797]\n",
      "[ 0.16103589  0.64569924 -0.15564106 -1.16294806]\n",
      "[ 0.17394988  0.45290797 -0.17890002 -0.9228307 ]\n",
      "[ 0.18300804  0.26059505 -0.19735663 -0.69128178]\n",
      "Episode finished after 20 timesteps\n",
      "[ 0.03736912 -0.02094721  0.03359308 -0.01950794]\n",
      "[ 0.03695017  0.17367728  0.03320292 -0.30140545]\n",
      "[ 0.04042372  0.36831065  0.02717481 -0.58343473]\n",
      "[ 0.04778993  0.17281875  0.01550612 -0.28231673]\n",
      "[ 0.05124631  0.36771613  0.00985978 -0.57006905]\n",
      "[ 0.05860063  0.1724573  -0.0015416  -0.27429631]\n",
      "[ 0.06204978  0.36760121 -0.00702752 -0.56746506]\n",
      "[ 0.0694018   0.17257854 -0.01837683 -0.27700435]\n",
      "[ 0.07285337  0.36795778 -0.02391691 -0.57542621]\n",
      "[ 0.08021253  0.17317913 -0.03542544 -0.29037269]\n",
      "[ 0.08367611 -0.02142025 -0.04123289 -0.00906967]\n",
      "[ 0.0832477   0.17426804 -0.04141428 -0.31447165]\n",
      "[ 0.08673307  0.36995471 -0.04770372 -0.61992229]\n",
      "[ 0.09413216  0.56570932 -0.06010216 -0.92723974]\n",
      "[ 0.10544635  0.76158905 -0.07864696 -1.23818822]\n",
      "[ 0.12067813  0.95762822 -0.10341072 -1.55443638]\n",
      "[ 0.13983069  1.15382637 -0.13449945 -1.87751003]\n",
      "[ 0.16290722  0.96040348 -0.17204965 -1.62941975]\n",
      "[ 0.18211529  0.76767046 -0.20463805 -1.39492192]\n",
      "Episode finished after 19 timesteps\n",
      "[ 0.0447305  -0.01836555 -0.00912598  0.03896485]\n",
      "[ 0.04436318  0.17688607 -0.00834668 -0.25658339]\n",
      "[ 0.04790091 -0.01811572 -0.01347835  0.03345522]\n",
      "[ 0.04753859  0.17719689 -0.01280925 -0.26344961]\n",
      "[ 0.05108253 -0.0177399  -0.01807824  0.02516578]\n",
      "[ 0.05072773  0.17763658 -0.01757492 -0.27316577]\n",
      "[ 0.05428046  0.37300483 -0.02303824 -0.57133966]\n",
      "[ 0.06174056  0.17821339 -0.03446503 -0.28600268]\n",
      "[ 0.06530483 -0.01640051 -0.04018509 -0.00438589]\n",
      "[ 0.06497682 -0.2109238  -0.0402728   0.27535224]\n",
      "[ 0.06075834 -0.01525109 -0.03476576 -0.02975584]\n",
      "[ 0.06045332 -0.20985767 -0.03536088  0.25175857]\n",
      "[ 0.05625617 -0.01424909 -0.03032571 -0.05186481]\n",
      "[ 0.05597118 -0.20892338 -0.031363    0.23109797]\n",
      "[ 0.05179272 -0.01336764 -0.02674104 -0.0713106 ]\n",
      "[ 0.05152536 -0.20809621 -0.02816725  0.2128169 ]\n",
      "[ 0.04736344 -0.40280436 -0.02391092  0.49648319]\n",
      "[ 0.03930735 -0.20735355 -0.01398125  0.19636164]\n",
      "[ 0.03516028 -0.40227275 -0.01005402  0.48460151]\n",
      "[ 0.02711483 -0.20701037 -0.00036199  0.18876691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02297462 -0.01188324  0.00341335 -0.10403019]\n",
      "[ 0.02273695  0.18318963  0.00133275 -0.39563426]\n",
      "[ 0.02640075 -0.01195121 -0.00657994 -0.10253144]\n",
      "[ 0.02616172  0.18326442 -0.00863057 -0.39728304]\n",
      "[ 0.02982701  0.37850775 -0.01657623 -0.69267447]\n",
      "[ 0.03739717  0.5738557  -0.03042972 -0.99052933]\n",
      "[ 0.04887428  0.76937142 -0.05024031 -1.2926121 ]\n",
      "[ 0.06426171  0.5749228  -0.07609255 -1.01607163]\n",
      "[ 0.07576016  0.77097235 -0.09641398 -1.33164464]\n",
      "[ 0.09117961  0.96716889 -0.12304687 -1.6528745 ]\n",
      "[ 0.11052299  0.77367999 -0.15610436 -1.40092009]\n",
      "[ 0.12599659  0.58080406 -0.18412277 -1.16083263]\n",
      "[ 0.13761267  0.7777835  -0.20733942 -1.5051343 ]\n",
      "Episode finished after 33 timesteps\n",
      "[0.01525798 0.0216394  0.00869285 0.01469545]\n",
      "[ 0.01569077  0.21663561  0.00898676 -0.27523215]\n",
      "[ 0.02002348  0.41162819  0.00348212 -0.56506715]\n",
      "[ 0.02825605  0.60670112 -0.00781923 -0.85665102]\n",
      "[ 0.04039007  0.80192873 -0.02495225 -1.15178232]\n",
      "[ 0.05642864  0.60714107 -0.04798789 -0.86702697]\n",
      "[ 0.06857146  0.41270383 -0.06532843 -0.5898099 ]\n",
      "[ 0.07682554  0.60867676 -0.07712463 -0.90233513]\n",
      "[ 0.08899908  0.80475404 -0.09517133 -1.21822891]\n",
      "[ 0.10509416  1.00096542 -0.11953591 -1.53915225]\n",
      "[ 0.12511347  1.19730549 -0.15031896 -1.86662068]\n",
      "[ 0.14905958  1.00411496 -0.18765137 -1.62413022]\n",
      "Episode finished after 12 timesteps\n",
      "[ 0.03535831 -0.04009962 -0.01465292  0.02390131]\n",
      "[ 0.03455632 -0.23500841 -0.0141749   0.31192529]\n",
      "[ 0.02985615 -0.03968741 -0.00793639  0.01480595]\n",
      "[ 0.0290624  -0.23469465 -0.00764027  0.30497432]\n",
      "[ 0.02436851 -0.03946466 -0.00154079  0.00989166]\n",
      "[ 0.02357922 -0.23456448 -0.00134295  0.30208806]\n",
      "[ 0.01888793 -0.42966726  0.00469881  0.59434714]\n",
      "[ 0.01029458 -0.23461139  0.01658575  0.30314801]\n",
      "[ 0.00560235 -0.42996575  0.02264871  0.60101523]\n",
      "[-0.00299696 -0.23516783  0.03466902  0.31555144]\n",
      "[-0.00770032 -0.43076602  0.04098004  0.61896308]\n",
      "[-0.01631564 -0.62643569  0.05335931  0.92426592]\n",
      "[-0.02884435 -0.43207354  0.07184462  0.64881783]\n",
      "[-0.03748582 -0.62811891  0.08482098  0.96323143]\n",
      "[-0.0500482  -0.43423272  0.10408561  0.6983552 ]\n",
      "[-0.05873286 -0.63063209  0.11805271  1.02190583]\n",
      "[-0.0713455  -0.43726378  0.13849083  0.7684991 ]\n",
      "[-0.08009077 -0.63399273  0.15386081  1.10135341]\n",
      "[-0.09277063 -0.83076708  0.17588788  1.43808493]\n",
      "[-0.10938597 -0.6381943   0.20464958  1.20512723]\n",
      "Episode finished after 20 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Viewer.__del__ of <gym.envs.classic_control.rendering.Viewer object at 0x000001BE3C35C358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 165, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 83, in close\n",
      "    self.window.close()\n",
      "  File \"c:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 299, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"c:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 823, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"c:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\_weakrefset.py\", line 109, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: (<weakref at 0x000001BE2CD5A048; to 'Win32Window' at 0x000001BE3C3722B0>,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MountainCarEnv' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5d43c265e5c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MountainCar-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_geom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         self.cartrans.set_translation(\n\u001b[0;32m    171\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_position\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_height\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MountainCarEnv' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45168115 -0.00053916]\n",
      "[-0.45275551 -0.00107436]\n",
      "[-0.45435721 -0.0016017 ]\n",
      "[-0.45647449 -0.00211728]\n",
      "[-0.45909181 -0.00261732]\n",
      "[-0.46218992 -0.00309811]\n",
      "[-0.46574599 -0.00355607]\n",
      "[-0.46973378 -0.00398779]\n",
      "[-0.47412381 -0.00439003]\n",
      "[-0.47888354 -0.00475973]\n",
      "[-0.48397762 -0.00509409]\n",
      "[-0.48936817 -0.00539055]\n",
      "[-0.49501499 -0.00564682]\n",
      "[-0.50087593 -0.00586094]\n",
      "[-0.50690716 -0.00603123]\n",
      "[-0.51306352 -0.00615636]\n",
      "[-0.51929889 -0.00623537]\n",
      "[-0.5255665  -0.00626761]\n",
      "[-0.53181936 -0.00625286]\n",
      "[-0.53801056 -0.00619121]\n",
      "[-0.54409372 -0.00608315]\n",
      "[-0.55002325 -0.00592954]\n",
      "[-0.55575482 -0.00573156]\n",
      "[-0.56124558 -0.00549076]\n",
      "[-0.56645459 -0.00520901]\n",
      "[-0.57134307 -0.00488848]\n",
      "[-0.5758747  -0.00453162]\n",
      "[-0.58001585 -0.00414116]\n",
      "[-0.5837359  -0.00372005]\n",
      "[-0.58700736 -0.00327146]\n",
      "[-0.58980612 -0.00279876]\n",
      "[-0.59211158 -0.00230546]\n",
      "[-0.5939068  -0.00179523]\n",
      "[-0.59517862 -0.00127182]\n",
      "[-0.59591771 -0.00073908]\n",
      "[-5.96118644e-01 -2.00938021e-04]\n",
      "[-5.95779964e-01  3.38680482e-04]\n",
      "[-0.59490415  0.00087582]\n",
      "[-0.59349761  0.00140654]\n",
      "[-0.59157066  0.00192695]\n",
      "[-0.58913745  0.00243321]\n",
      "[-0.58621586  0.00292159]\n",
      "[-0.5828274   0.00338846]\n",
      "[-0.57899706  0.00383034]\n",
      "[-0.57475314  0.00424392]\n",
      "[-0.57012707  0.00462607]\n",
      "[-0.56515316  0.0049739 ]\n",
      "[-0.55986841  0.00528475]\n",
      "[-0.55431217  0.00555624]\n",
      "[-0.54852591  0.00578626]\n",
      "[-0.54255286  0.00597304]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9c9eda7e14a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "MAX_NUM_EPISODES = 5\n",
    "\n",
    "for episode in range(MAX_NUM_EPISODES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0 # To keep track of the total reward obtained in each episode\n",
    "    step = 0\n",
    "    i = 1\n",
    "    action = env.action_space.sample()\n",
    "    while not done:\n",
    "        i+=1\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        \n",
    "        #action = env.action_space.sample()  # Sample random action.\n",
    "                                            # This will be replaced\n",
    "                                            # by our agent's action\n",
    "                                            # when we # start\n",
    "                                            # developing the agent algorithms\n",
    "        \n",
    "        next_state, reward, done, info = \\\n",
    "        env.step(action)  # Send the action to the\n",
    "                          # environment and receive       \n",
    "                          # the next_state, reward and\n",
    "                          # whether done or not\n",
    "        total_reward += reward\n",
    "        if (i != 0):\n",
    "            print(next_state)\n",
    "        step += 1\n",
    "        #obs = np.array([0, 0])\n",
    "    print(\"\\n Episode #{} ended in {} steps. total_reward={} done = {}\".format(episode, step+1,\n",
    "total_reward, done))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space\n",
    "print(state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-Learning - это метод основанный на представлении функции полезности Q(s, a) в виде таблицы. Соответственно, такой метод применим только для дискретного набора действий и дискретного количества состояний среды, причём желательно, чтобы число возможных действия и число состояний было небольшим. Ограниченно этот метод можно применить к средам с непрерывным состоянием, если его искусственно дискретизировать.\n",
    "\n",
    "\n",
    "\n",
    "Если ввести функцию дисконтированного будущего вознаграждения:\n",
    "\n",
    "\\\\[ R_t = \\sum_{k=0}^{\\infty}{\\gamma^{k} r_{t+k+1}}, \\quad где \\quad \\gamma \\in (0, 1] \\\\]\n",
    "\n",
    "то можно определить функцию \\\\( Q(s, a) \\\\) как математическое ожидание будущего вознаграждения при выполнении действий \\\\( a \\\\) в состоянии \\\\( s \\\\),\n",
    "\n",
    "\\\\[ Q(S, A) = max_{\\pi} \\mathbb{E} [G_t | S_t = s, A_t = a, \\pi] \\\\]\n",
    "\n",
    "SARSA (State, Action, Reward, State, Action) /* on-policy */:\n",
    "\n",
    "\\\\[ Q(S_t, A_t) = (1 - \\alpha) Q(S_t, A_t) + \\alpha [r_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})] \\\\]\n",
    "\n",
    "\\\\[ Q(S_t, A_t) = Q(S_t, A_t) + \\alpha [r_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)] \\\\]\n",
    "\n",
    "\\\\[ TD_{target} = r + \\gamma Q(S_{t+1},A_{t+1})\\\\]\n",
    "\n",
    "\\\\[ TD_{error} = TD_{target} - Q(S_t, A_t) \\\\]\n",
    "\n",
    "Q-обучение /* off-policy */:\n",
    "\n",
    "Уравнение Беллмана:\n",
    "\n",
    "\\\\[ Q(S_t,A_t) = Q(S_t, A_t) + \\alpha [r_{t+1} +  \\gamma \\cdot max_{a}{Q(S_{t+1},a}) - Q(S_t, A_t) ] \\\\]\n",
    "\n",
    "\\\\[ TD_{target} = r + \\gamma max_{a} Q(S_{t+1},a)\\\\]\n",
    "\n",
    "\\\\[ TD_{error} = TD_{target} - Q(S_t, A_t) \\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эпсилон-жадный алгоритм\n",
    "Для того, чтобы начать стоить функцию оценки награды для комбинации состояния и действия, нужно какое-то действие выбрать, а пока м ы еще ничего не знаем. Поэтому можно выбрать несколько эпизодов случайное действие. Это будет фазой исследования. Далее можно попробовать выбирать действия, которые ведут в увеличению награды, но тогда выбирать мы будем только из уже известных действий, которые мы исследовали. Получается, что обучение на этом закончится. Лучше если мы будем комбинировать исследование и использование накопленного опыта. Введём эпсилон, как величину от 0 до 1, которая определяет вероятность случаного выбора действия на каждом шаге. Это называется эпсилон-жадным алгоритмом. Выбор значения эпсилон также важен. 0 и 1 явно не подходят, как крайности. 0.5 это тоже странное поведение, когда каждое второе действие делается без царя в голове - случайным образом. А вот значения около 0.1 выглядят разумными.\n",
    "\n",
    "Для задачи многорукого бандита (набор игровых автоматов, у которых случайным образом есть вероятность выигрыша по нормальному распределению) нужно научиться выигрывать, не зная заранее вероятность выигрыша. При разных эпсилон получились такие графики:\n",
    "<img src=\"e-gready_chart.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим с помощью Q-learning предыдущий пример с Mountain-car. При этом гамма `GAMMA` это наш дисконт, экспоненциально спадающий из будущего в прошлое с каждым шагом по времени, а время у нас разбито на шаги в Gym. Состояние в виде координаты нужно сделать дискретным, так как с непрерывными состояниями Q-learning не работает. Для этого введём `NUM_DISCRETE_BINS`, на которые мы разобьём координаты. Для каждого положения нужно понять какое решение лучше. Наконец есть `ALPHA`, который скорость обучения. Это аналог масштабного коэффициента при градиентном спуске. Наконец здесь используется автоподстройка для эпсилон-жадного алгоритма, когда нет фазы изначального исследования среды, а затем переход на фиксированную вероятность случайного действия. Вместо этого сам эпсилон начинает экспоненциально затухать от 1 до некоторого минимального значения (`EPSILON_MIN`) со скоростью `EPSILON_DECAY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#:19999 reward:-183.0 best_reward:-119.0 eps:0.033631999948726646"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "EPSILON_MIN = 0.005\n",
    "MAX_NUM_EPISODES = 20000\n",
    "MAX_STEPS_PER_EPISODE = 500\n",
    "max_num_steps = MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE\n",
    "EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\n",
    "ALPHA = 0.05 # learning rate\n",
    "GAMMA = 0.95 # Discount factor\n",
    "NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each\n",
    "                        # observation dim\n",
    "\n",
    "class QAgent():\n",
    "    def __init__(self, env):\n",
    "        self.obs_shape = env.observation_space.shape\n",
    "        self.obs_high = env.observation_space.high\n",
    "        self.obs_low = env.observation_space.low\n",
    "        self.obs_bins = NUM_DISCRETE_BINS\n",
    "        self.bin_width = (self.obs_high - self.obs_low) \\\n",
    "            / self.obs_bins\n",
    "        self.action_shape = env.action_space.n\n",
    "        # Create a table to represent the Q-values\n",
    "        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,\n",
    "                          self.action_shape)) # (31 x 31 x 3)\n",
    "        self.alpha = ALPHA\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = 1.0\n",
    "    \n",
    "    \n",
    "    def discretize(self, obs):\n",
    "        return tuple(((obs - self.obs_low) \\\n",
    "                      / self.bin_width).astype(int))\n",
    "    \n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        discretized_obs  = self.discretize(obs)\n",
    "        # Epsilon-Greedy action selection\n",
    "        if self.epsilon > EPSILON_MIN:\n",
    "            self.epsilon -= EPSILON_DECAY\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return np.argmax(self.Q[discretized_obs])\n",
    "        else:  # choose a random action\n",
    "            return np.random.choice([a for a in range(self.action_shape)])\n",
    "        \n",
    "    \n",
    "    def learn(self, obs, action, reward, next_obs):\n",
    "        discretized_obs = self.discretize(obs)\n",
    "        discretized_next_obs = self.discretize(next_obs)\n",
    "        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])\n",
    "        td_error = td_target - self.Q[discretized_obs][action]\n",
    "        self.Q[discretized_obs][action] +=  self.alpha * td_error\n",
    "\n",
    "\n",
    "def train_Q(agent, env):\n",
    "    best_reward = -float('inf')\n",
    "    for episode in range(MAX_NUM_EPISODES):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        total_reward = 0.0\n",
    "        while not done:\n",
    "            action = agent.get_action(obs)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "        best_reward = max(best_reward, total_reward)\n",
    "        print(\"\\rEpisode#:{} reward:{} best_reward:{} eps:{}\"\n",
    "             .format(episode, total_reward, best_reward, agent.epsilon), end=\"\")\n",
    "        \n",
    "    # Return the trained policy\n",
    "    return np.argmax(agent.Q, axis=2)\n",
    "\n",
    "\n",
    "def test_Q(agent, env, policy):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    while not done:\n",
    "        action = policy[agent.discretize(obs)]\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    agent = QAgent(env)\n",
    "    learned_policy = train_Q(agent, env)\n",
    "    # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "    gym_monitor_path = \"./output/gym\"\n",
    "    env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n",
    "    for _ in range(1000):\n",
    "        test_Q(agent, env, learned_policy)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Networks\n",
    "\n",
    "Таблицу, аппроксимирующую нашу функцию качества можно заменить нейросетью, которая будет предсказывать значению Q сразу для всех возможных действий. В этом случае нам нужно определить функцию потерь, от которой мы будем считать градиент.\n",
    "\n",
    "\\\\[ L = {TD_{error}}^2 \\\\]\n",
    "\\\\[ L = (r + \\gamma max_a Q(S_t, a) - Q(S_t, A_t))^2 \\\\]\n",
    "\n",
    "## Experience replay\n",
    "\n",
    "В большинстве окружений информация, получаемая агентом распределена не независимо. Т.е. последовательные наблюдения агента сильно коррелированы между собой (что понятно из интуитивных соображений, т.к. большинство окружений, в которых применяется RL, предполагают, что все изменения в них последовательны). Корреляция примеров ухудшает сходимость стохастического градиентного спуска. Таким образом нам нужен способ, который позволяет улучшить распределение примеров для обучения (устранить или снизить корреляцию между ними). Обычно используется метод **проигрывания опыта (experience replay)**. Суть этого метода в том, что мы сохраняем некоторое количество примеров (состояние, действия, вознаграждение) в специальном буфере и для обучения выбираем случайные мини-батчи из этого буфера.\n",
    "\n",
    "Так же **experience replay** позволяет агенту эффективнее использовать свой прошлый опыт.\n",
    "\n",
    "## Double DQN\n",
    "\n",
    "Одной из проблем Q-Networks является неустойчивость. Часто разность ожидаемых вознаграждений для различных действий близка и поскольку выбор действия производится с помощью argmax, то выброс в данных может привести к тому, что выбираемое действие изменится. Для того, чтобы повысить стабильность используется техника **Target Q-Network**. Суть в том, что мы замораживаем веса нашей сети на фиксированное число шагов и затем используем её для вычисления функции ошибки и обучения второй сети. Периодически копируем из веса рабочей сети в Target Q-Network.\n",
    "В следующем примере мы будем делать это каждый эпизод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 132.8603607867613   memory length: 200   epsilon: 0.818648829478636\n",
      "episode: 1   score: 258.99874864916046   memory length: 400   epsilon: 0.6701859060067403\n",
      "episode: 2   score: 64.21063252956688   memory length: 600   epsilon: 0.5486469074854965\n",
      "episode: 3   score: -23.58795208810089   memory length: 800   epsilon: 0.4491491486100748\n",
      "episode: 4   score: 145.17677297661072   memory length: 1000   epsilon: 0.3676954247709635\n",
      "episode: 5   score: -91.4321129765998   memory length: 1000   epsilon: 0.3010134290933992\n",
      "episode: 6   score: -49.58567160036901   memory length: 1000   epsilon: 0.24642429138466176\n",
      "episode: 7   score: 109.78082578526909   memory length: 1000   epsilon: 0.20173495769715546\n",
      "episode: 8   score: 859.0836638780509   memory length: 1000   epsilon: 0.1653154023860845\n",
      "episode: 9   score: 355.50505003213806   memory length: 1000   epsilon: 0.13533526065815754\n",
      "episode: 10   score: -47.96968236651804   memory length: 1000   epsilon: 0.11079205272498677\n",
      "episode: 11   score: 135.712061629252   memory length: 1000   epsilon: 0.0906997842788457\n",
      "episode: 12   score: 568.6187537917002   memory length: 1000   epsilon: 0.07425127223384173\n",
      "episode: 13   score: 708.6933260495454   memory length: 1000   epsilon: 0.06532604899029316\n",
      "episode: 14   score: 516.6917909988335   memory length: 1000   epsilon: 0.053479093540367534\n",
      "episode: 15   score: 470.70553111459446   memory length: 1000   epsilon: 0.043780597328400374\n",
      "episode: 16   score: 843.9028663054718   memory length: 1000   epsilon: 0.03886644166514151\n",
      "episode: 17   score: 869.6059982898446   memory length: 1000   epsilon: 0.034434902439230815\n",
      "episode: 18   score: 1226.3238528025897   memory length: 1000   epsilon: 0.028559141954960843\n",
      "episode: 19   score: 1284.906715962133   memory length: 1000   epsilon: 0.023544224095529885\n",
      "episode: 20   score: 889.3165778044312   memory length: 1000   epsilon: 0.020283464132356634\n",
      "episode: 21   score: 890.1027000184785   memory length: 1000   epsilon: 0.018042813118524962\n",
      "episode: 22   score: 947.1698564575668   memory length: 1000   epsilon: 0.01601759652309025\n",
      "episode: 23   score: 1018.6460358366031   memory length: 1000   epsilon: 0.014021913505049215\n",
      "episode: 24   score: 617.015259472982   memory length: 1000   epsilon: 0.012763305867568336\n",
      "episode: 25   score: 1046.1247656495088   memory length: 1000   epsilon: 0.010973670081346181\n",
      "episode: 26   score: 795.3529172321829   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 27   score: 975.6759802973156   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 28   score: 1345.2639293502825   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 29   score: 1137.1360196020858   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 30   score: 1216.1498248836415   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 31   score: 588.0958411295704   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 32   score: 614.6377623966825   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 33   score: 1296.3014622067662   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 34   score: 1024.5591158556974   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 35   score: 858.9103813832074   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 36   score: 1085.5596984185947   memory length: 1000   epsilon: 0.009998671593271896\n",
      "episode: 37   score: 1161.3554361427457   memory length: 1000   epsilon: 0.009998671593271896\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "EPISODES = 300\n",
    "\n",
    "\n",
    "# Double DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 500\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=1000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.model.load_weights(path)\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "        \n",
    "        target = self.model.predict(update_input)\n",
    "        target_next = self.model.predict(update_target)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_next[i])\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    target_val[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, you can play until 500 time step\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    # get size of state and action from environment\n",
    "    \n",
    "     \n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DoubleDQNAgent(state_size, action_size)\n",
    "    \n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            f = 500*(next_state[0][0] - state[0][0]) if ((next_state[0][0] - state[0][0]) > 0) else 0\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = (reward+f) if not done or score == 499 else -100\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            agent.train_model()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "\n",
    "                # every episode, plot the play time and remove penalty introduced above\n",
    "                score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./output/cartpole_ddqn.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "        # if the mean of scores of last 10 episode is bigger than 490\n",
    "        # stop training\n",
    "        if np.mean(scores[-min(20, len(scores)):]) > 1000:\n",
    "            agent.model.save_weights(\"./output/cartpole_ddqn.h5\")\n",
    "            break\n",
    "\n",
    "        # save the model each 50 episodes\n",
    "        if e % 50 == 0:\n",
    "            agent.model.save_weights(\"./output/cartpole_ddqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация нескольких видеозаписей решения задачи среды обученным агентом. Создаём новый экземпляр класса, загружаем в него сохранённые данные обучения (в данном случае нейронную сеть), убираем эпсилон, чтобы новый опыт не собирался. Тут аналогия с Dropout слоем, он тоже отключается на inference. Включаем монитор для записи и запускаем цикл действий агента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total reward:  -186.0   Iterations:  186\n",
      "Total reward:  -191.0   Iterations:  191\n",
      "Total reward:  -187.0   Iterations:  187\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        env = gym.make('MountainCar-v0')\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = DoubleDQNAgent(state_size, action_size)\n",
    "        agent.load_model(\"./output/cartpole_ddqn.h5\")\n",
    "        agent.epsilon = 0\n",
    "\n",
    "        # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "        gym_monitor_path = \"./output/gym\"\n",
    "        # Record every episode, instead of selective\n",
    "        env = gym.wrappers.Monitor(env, gym_monitor_path, video_callable=lambda episode_id: True, force=True)\n",
    "        for _ in range(3):\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                obs = np.reshape(obs, [1, state_size])\n",
    "                total_reward = 0.0\n",
    "                iteration = 0\n",
    "                while not done:\n",
    "                    action = agent.get_action(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    next_obs = np.reshape(next_obs, [1, state_size])\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "                    iteration += 1\n",
    "                print(\"Total reward: \", total_reward, \"  Iterations: \", iteration)\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Другие улучшения DQN\n",
    "\n",
    "## Prioritized Experience Replay\n",
    "\n",
    "Минибатчи из памяти выбираются не с равномерным распределением, а добавляем туда больше примеров, в которых предсказанные значения Q сильнее всего отличаются от корректных. Т.е. примеры с максимальным **TD error** получают максимальный приоритет.\n",
    "\n",
    "## Dueling networks\n",
    "\n",
    "Основная идея в том, что мы разделяем нашу сеть на две головы, одна из которых предсказывает абсолютное значение состояния \\\\( V(S) \\\\), а вторая - относительное преимущество одний действий над другими \\\\( A(s, a) = Q(s, a) - V(s) \\\\). Это преимущество называется advantage. Далее из этих двух значений мы собираем нашу Q-функцию, как \\\\( Q(s,a) = V(s) + A(a) \\\\)\n",
    "\n",
    "## Noisy nets\n",
    "\n",
    "Т.к. по мере обучения агент будет стремиться выбирать состояния с максимальным Q, среди уже исследованных, это может помешать ему найти более эффективные состояния, в которых его ещё не было. Одним из решений этой проблемы является использование детерминированной и случайной нейросети, распределение параметров которой так же обучается с помощью градиентного спуска.\n",
    "\n",
    "## Multi-step learning/n-step learning\n",
    "\n",
    "Основная идея в том, чтобы считать функцию ценности не по двум соседним примерам, а сразу по n. Это позволяет сети лучше запоминать длинные последовательности действий.\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "Детерминированное значение Q заменяется случайным распределением Z с некоторыми параметрами, которые определяются в ходе обучения.\n",
    "\n",
    "# Rainbow\n",
    "\n",
    "State of the art в развитии Q-обучения - набор перечисленных выше твиков. На графике ниже сравнение различных алгоритмов по количеству очков, усреднённое по играм Atari в сравнении со средними результатами человека.\n",
    "\n",
    "<img src=\"rainbow_dqn.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients\n",
    "\n",
    "Мы используем нейросеть, которая получает на вход вектор состояния среды, а на выход сразу выдаёт вектор необходимых действий. Такой подход называется DPG (Deep Policy Gradients) или, в случае детерминированной стратегии, DDPG (Deep Deterministic Policy Gradients). Для подстройки весов в этом случае мы будем использовать градиент:\n",
    "\\\\[ - \\nabla log P(s, a, \\theta) R \\\\]\n",
    "где P - предсказание вероятности действий нейросетью, а R - полученное вознаграждение.\n",
    "\n",
    "Основной минус этого метода - необходимо дожидаться конца эпизода для получения куммулятивного вознаграждения. В отличии от DQN возможна работа с непрерывными действиями, тогда, как DQN может работать только с дискретным набором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "\n",
    "# This is Policy Gradient agent for the Cartpole\n",
    "# In this example, we use DPG algorithm which uses monte-carlo update rule\n",
    "class DPGAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.hidden1, self.hidden2 = 24, 24\n",
    "\n",
    "        # create model for policy network\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        # lists for the states, actions and rewards\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.model.load_weights(path)\n",
    "\n",
    "    # approximate policy using Neural Network\n",
    "    # state is input and probability of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden1, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "        model.add(Dense(self.hidden2, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='softmax', kernel_initializer='glorot_uniform'))\n",
    "        model.summary()\n",
    "        # Using categorical crossentropy as a loss is a trick to easily\n",
    "        # implement the policy gradient. Categorical cross entropy is defined\n",
    "        # H(p, q) = sum(p_i * log(q_i)). For the action taken, a, you set \n",
    "        # p_a = advantage. q_a is the output of the policy network, which is\n",
    "        # the probability of taking the action a, i.e. policy(s, a). \n",
    "        # All other p_i are zero, thus we have H(p, q) = A * log(policy(s, a))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.model.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # In Policy Gradient, Q function is not available.\n",
    "    # Instead agent uses sample returns for evaluating policy\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save <s, a ,r> of each step\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self):\n",
    "        episode_length = len(self.states)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards(self.rewards)\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        update_inputs = np.zeros((episode_length, self.state_size))\n",
    "        advantages = np.zeros((episode_length, self.action_size))\n",
    "\n",
    "        for i in range(episode_length):\n",
    "            update_inputs[i] = self.states[i]\n",
    "            advantages[i][self.actions[i]] = discounted_rewards[i]\n",
    "\n",
    "        self.model.fit(update_inputs, advantages, epochs=1, verbose=0)\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, you can play until 500 time step\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # make DPG agent\n",
    "    agent = DPGAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            # save the sample <s, a, r> to the memory\n",
    "            agent.append_sample(state, action, reward)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode, agent learns from sample returns\n",
    "                agent.train_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./output/cartpole_dpg.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "        # if the mean of scores of last 10 episode is bigger than 490\n",
    "        # stop training\n",
    "        if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "            agent.model.save_weights(\"./output/cartpole_dpg.h5\")\n",
    "            break\n",
    "\n",
    "        # save the model each 50 episodes\n",
    "        if e % 50 == 0:\n",
    "            agent.model.save_weights(\"./output/cartpole_dpg.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация нескольких видеозаписей решения задачи среды обученным агентом. Создаём новый экземпляр класса, загружаем в него сохранённые данные обучения (в данном случае нейронную сеть), убираем эпсилон, чтобы новый опыт не собирался. Включаем монитор для записи и запускаем цикл действий агента. Всё, как в методе DDQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = DPGAgent(state_size, action_size)\n",
    "        agent.load_model(\"./output/cartpole_dpg.h5\")\n",
    "        agent.epsilon = 0\n",
    "\n",
    "        # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "        gym_monitor_path = \"./output/gym\"\n",
    "        env = gym.wrappers.Monitor(env, gym_monitor_path, video_callable=lambda episode_id: True, force=True)\n",
    "        for _ in range(3):\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                obs = np.reshape(obs, [1, state_size])\n",
    "                total_reward = 0.0\n",
    "                while not done:\n",
    "                    action = agent.get_action(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    next_obs = np.reshape(next_obs, [1, state_size])\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшения метода Policy Gradients\n",
    "\n",
    "Такого большого набора твиков, как для DQN для DPG не наблюдается. Наиболее известны методы: TRPO (Trust Region Policy Optimization) и PPO (Proximal Policy Optimization). В них заложена несколько разная математика, но суть обоих методов в ограничении изменения весов за один прогон для того, чтобы выбросы не портили выученную стратегию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-critic и его модификации\n",
    "\n",
    "Можно взять две сети, одна из которых будет предсказывать действия, а вторая - оценивать, насколько эти действия хороши, т.е. выдавать значение Q. Помимо самих действий ей на вход мы так же подадим состояне.\n",
    "\n",
    "<img src=\"actor_critic.png\">\n",
    "\n",
    "Плюс в том, что нам не обязательно дожидаться окончания эпизода для обучения.\n",
    "\n",
    "### Advantage-Actor-Critic, A2C\n",
    "\n",
    "Вместо вычисления градиентов от абсолютного значения \\\\( Q(s, a) \\\\) мы можем использовать относительное преимущество одний действия над другими \\\\( A(s, a) = Q(s, a) - V(s) \\)). Тогда если \\\\( A(s, a) > 0 \\\\), то градиентный спуск будет изменять все веса, повышая вероятность предсказанных действий. Если же \\\\( A(s, a) < 0 \\\\), то градиентный спуск будет понижать вероятность таких действий. \\\\( V(s) \\\\) при этом показывает, насколько состояние хорошо само по себе: если мы в двух шагах от вершины Эвереста, то это очень хороший state, а если мы летим в пропасть, то state крайне фиговый, чтобы мы в нём не делали (если, конечно, у нас нет с собой парашюта).\n",
    "\n",
    "При использовании такого подхода функцию Q(s, a) можно заменить прямо на полученное вознаграждение за некоторое действие вознаграждение r. Тогда \\\\( A(s,a) = r - V(s) \\\\). При этом получается, что сети Actor и Critic можно объединить в одну с двумя головами, что улучшает переиспользование весов и ускоряет обучение.\n",
    "\n",
    "<img src=\"a2c.png\" width=700>\n",
    "\n",
    "### Asynchronous Advantage Actor Critic A3C\n",
    "У нас есть сервер, собирающий результаты с нескольких Actor'ов и обновляющий веса, когда наберётся батч достаточного размера. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 147\n",
      "Trainable params: 147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-09b1f79513bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m499\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-09b1f79513bc>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0madvantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mnext_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mc:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1577\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1579\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;31m# 5. disabled static optimizations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m       \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mrange\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m     \"\"\"\n\u001b[1;32m--> 955\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mRangeDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3576\u001b[0m         \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3577\u001b[0m         \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3578\u001b[1;33m         **self._flat_structure)\n\u001b[0m\u001b[0;32m   3579\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRangeDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chkn2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mrange_dataset\u001b[1;34m(start, stop, step, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   5318\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RangeDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5319\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5320\u001b[1;33m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   5321\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5322\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "\n",
    "# A2C(Advantage Actor-Critic) agent for the Cartpole\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.001\n",
    "        self.critic_lr = 0.005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        def load_model(self, path_actor, path_critic):\n",
    "            self.actor.load_weights(path_actor)\n",
    "            self.critic.load_weights(path_critic)\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # make A2C agent\n",
    "    agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500.0 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./output/cartpole_a2c.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "        # if the mean of scores of last 10 episode is bigger than 490\n",
    "        # stop training\n",
    "        if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "            break\n",
    "\n",
    "        # save the model every 50 episodes\n",
    "        if e % 50 == 0:\n",
    "            agent.actor.save_weights(\"./output/cartpole_actor.h5\")\n",
    "            agent.critic.save_weights(\"./output/cartpole_critic.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = A2CAgent(state_size, action_size)\n",
    "        agent.load_model(\"./output/cartpole_actor.h5\", \"./output/cartpole_critic.h5\")\n",
    "        agent.epsilon = 0\n",
    "\n",
    "        # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "        gym_monitor_path = \"./output/gym\"\n",
    "        env = gym.wrappers.Monitor(env, gym_monitor_path, video_callable=lambda episode_id: True, force=True)\n",
    "        for _ in range(3):\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                obs = np.reshape(obs, [1, state_size])\n",
    "                total_reward = 0.0\n",
    "                while not done:\n",
    "                    action = agent.get_action(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    next_obs = np.reshape(next_obs, [1, state_size])\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based\n",
    "\n",
    "Ещё один подход заключается в том, что мы можем обучать нейросеть предсказывать следующее состояние среды, подавая ей на вход действия и предыдущее состояние. Таким образом нейросеть учит поведение среды. Для того, чтобы выбрать оптимальные действия, нам придётся прогнать все возможные действия через предсказание нейросети, поэтому такой подход применим только при малой размерности пространства действий.\n",
    "\n",
    "## Imitation learning\n",
    "\n",
    "Для того, чтобы агент выучил сложную последовательность действий, можно искуственно поставить его в конец этой траектории, тогда он быстро выучит, как пройти небольшой участок. После этого его можно поставить чуть дальше и так, пока агент на научится выполнять всю последовательность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Использовать для тележки-машинки один из 3 методов обучения с покреплением, который был использован для маятника (DDQN, DPG, A2C). Придумать и применить reward-shaping для этого обучения и сравнить с обучением без него."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ссылки\n",
    "Про то, что RL часто плохо работает с примерами видео. Очень ценная ссылка, чтобы увидеть типовые ошибки RL: https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "\n",
    "DeepMind Has Quietly Open Sourced Three New Impressive Reinforcement Learning Frameworks: https://towardsdatascience.com/deepmind-quietly-open-sourced-three-new-impressive-reinforcement-learning-frameworks-f99443910b16\n",
    "\n",
    "Reinforcement Learning — Generalisation in Continuous State Space. Тут есть математика и алгоритмы в стиле Воронцова. Есть ссылки на более простой RL, что позволяет лучше понять материал. В целом скорее академический материал. https://towardsdatascience.com/reinforcement-learning-generalisation-in-continuous-state-space-df943b04ebfa\n",
    "\n",
    "Про эпсилон-жадный алгоритм довольно простым языком: https://medium.com/@congyuzhou/k-armed-бандит-и-greedy-алгоритм-6502891dbae6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
